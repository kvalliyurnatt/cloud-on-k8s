// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"

	"github.com/elastic/cloud-on-k8s/v3/hack/cve-triage/internal/deps"
	"github.com/elastic/cloud-on-k8s/v3/hack/cve-triage/internal/parser"
	"github.com/elastic/cloud-on-k8s/v3/hack/cve-triage/internal/vulncheck"
)

const (
	// DefaultOllamaURL is the default Ollama API endpoint
	DefaultOllamaURL = "http://localhost:11434"
	// DefaultOllamaModel is the default model to use with Ollama
	DefaultOllamaModel = "llama3.2"
)

// OllamaAnalyzer uses Ollama (local LLM) to analyze CVE impact on ECK
type OllamaAnalyzer struct {
	baseURL string
	model   string
	client  *http.Client
	verbose bool
}

// Ensure OllamaAnalyzer implements Provider interface
var _ Provider = (*OllamaAnalyzer)(nil)

// OllamaRequest is the request format for Ollama API
type OllamaRequest struct {
	Model   string  `json:"model"`
	Prompt  string  `json:"prompt"`
	Stream  bool    `json:"stream"`
	Options Options `json:"options,omitempty"`
}

// Options for Ollama generation
type Options struct {
	Temperature float64 `json:"temperature,omitempty"`
	TopP        float64 `json:"top_p,omitempty"`
}

// OllamaResponse is the response format from Ollama API
type OllamaResponse struct {
	Model    string `json:"model"`
	Response string `json:"response"`
	Done     bool   `json:"done"`
	Error    string `json:"error,omitempty"`
}

// NewOllamaAnalyzer creates a new Ollama-based analyzer
func NewOllamaAnalyzer(baseURL, model string) *OllamaAnalyzer {
	if baseURL == "" {
		baseURL = DefaultOllamaURL
	}
	if model == "" {
		model = DefaultOllamaModel
	}

	return &OllamaAnalyzer{
		baseURL: baseURL,
		model:   model,
		client: &http.Client{
			Timeout: 5 * time.Minute, // LLM can take a while
		},
	}
}

// Name returns the provider name
func (o *OllamaAnalyzer) Name() string {
	return fmt.Sprintf("Ollama (%s)", o.model)
}

// Close is a no-op for Ollama (no persistent connection)
func (o *OllamaAnalyzer) Close() error {
	return nil
}

// IsAvailable checks if Ollama is running
func (o *OllamaAnalyzer) IsAvailable(ctx context.Context) bool {
	req, err := http.NewRequestWithContext(ctx, "GET", o.baseURL+"/api/tags", nil)
	if err != nil {
		return false
	}

	resp, err := o.client.Do(req)
	if err != nil {
		return false
	}
	defer resp.Body.Close()

	return resp.StatusCode == http.StatusOK
}

// Analyze performs LLM-based analysis using Ollama
func (o *OllamaAnalyzer) Analyze(ctx context.Context, cveInfo *parser.CVEInfo, depResult *deps.CheckResult, vulnResult *vulncheck.Result) (*AnalysisResult, error) {
	prompt := buildAnalysisPrompt(cveInfo, depResult, vulnResult)
	return o.analyzeWithPrompt(ctx, prompt)
}

// AnalyzeWithCode performs LLM-based analysis with source code context
func (o *OllamaAnalyzer) AnalyzeWithCode(ctx context.Context, cveInfo *parser.CVEInfo, depResult *deps.CheckResult, vulnResult *vulncheck.Result, codeContext string) (*AnalysisResult, error) {
	prompt := BuildAnalysisPromptWithCode(cveInfo, depResult, vulnResult, codeContext)
	return o.analyzeWithPrompt(ctx, prompt)
}

// analyzeWithPrompt performs analysis with a custom prompt (used by RAG)
func (o *OllamaAnalyzer) analyzeWithPrompt(ctx context.Context, prompt string) (*AnalysisResult, error) {
	reqBody := OllamaRequest{
		Model:  o.model,
		Prompt: prompt,
		Stream: false,
		Options: Options{
			Temperature: 0.2,
			TopP:        0.8,
		},
	}

	jsonBody, err := json.Marshal(reqBody)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, "POST", o.baseURL+"/api/generate", bytes.NewReader(jsonBody))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := o.client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to call Ollama: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("Ollama returned status %d: %s", resp.StatusCode, string(body))
	}

	var ollamaResp OllamaResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if ollamaResp.Error != "" {
		return nil, fmt.Errorf("Ollama error: %s", ollamaResp.Error)
	}

	if o.verbose {
		fmt.Printf("\n=== RAW LLM RESPONSE ===\n%s\n=== END RAW RESPONSE ===\n\n", ollamaResp.Response)
	}

	return parseAnalysisResponse(ollamaResp.Response), nil
}

// GenerateRaw sends a prompt and returns the raw text response
func (o *OllamaAnalyzer) GenerateRaw(ctx context.Context, prompt string) (string, error) {
	reqBody := OllamaRequest{
		Model:  o.model,
		Prompt: prompt,
		Stream: false,
		Options: Options{
			Temperature: 0.2,
			TopP:        0.8,
		},
	}

	jsonBody, err := json.Marshal(reqBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, "POST", o.baseURL+"/api/generate", bytes.NewReader(jsonBody))
	if err != nil {
		return "", fmt.Errorf("failed to create request: %w", err)
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := o.client.Do(req)
	if err != nil {
		return "", fmt.Errorf("failed to call Ollama: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("ollama returned status %d: %s", resp.StatusCode, string(body))
	}

	var ollamaResp OllamaResponse
	if err := json.NewDecoder(resp.Body).Decode(&ollamaResp); err != nil {
		return "", fmt.Errorf("failed to decode response: %w", err)
	}

	if ollamaResp.Error != "" {
		return "", fmt.Errorf("ollama error: %s", ollamaResp.Error)
	}

	return ollamaResp.Response, nil
}

// ListModels returns available Ollama models
func (o *OllamaAnalyzer) ListModels(ctx context.Context) ([]string, error) {
	req, err := http.NewRequestWithContext(ctx, "GET", o.baseURL+"/api/tags", nil)
	if err != nil {
		return nil, err
	}

	resp, err := o.client.Do(req)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	var result struct {
		Models []struct {
			Name string `json:"name"`
		} `json:"models"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return nil, err
	}

	var models []string
	for _, m := range result.Models {
		models = append(models, m.Name)
	}
	return models, nil
}
