# RAG Implementation Plan: CodeBERT + File-Based Persistence

> **Status**: Planned (not yet implemented)
> **Approach**: Path A variant with CodeBERT embeddings and file-based persistence
> **Estimated Effort**: 2-3 days
> **Dependencies**: Hugging Face API (free tier available)

## Overview

Use Microsoft's CodeBERT model (via Hugging Face Inference API) to generate code-specific embeddings, stored in a simple JSON file for persistence. This gives better code understanding than generic text embeddings while keeping the solution simple and self-contained.

## Why This Approach

| Aspect | Generic Embeddings | CodeBERT |
|--------|-------------------|----------|
| **Training Data** | General text | 6M code functions + NL |
| **Languages** | N/A | Python, Java, Go, JS, Ruby, PHP |
| **Code Understanding** | Treats code as text | Understands code semantics |
| **Best For** | Natural language | Code similarity, search |

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   ECK CVE Triage with CodeBERT RAG                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                     Indexing Phase (One-time / Periodic)               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ   ECK    ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ   Go     ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ CodeBERT ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ eck-index.json ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  Source  ‚îÇ    ‚îÇ Chunker  ‚îÇ    ‚îÇ (HF API) ‚îÇ    ‚îÇ  (persisted)   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  Files   ‚îÇ    ‚îÇ  (AST)   ‚îÇ    ‚îÇ          ‚îÇ    ‚îÇ                ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                     Query Phase (Per CVE Triage)                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ eck-index.json ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  Cosine  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ   Top-K Code   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  (load once)   ‚îÇ         ‚îÇ Search   ‚îÇ         ‚îÇ    Chunks      ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                   ‚ñ≤                        ‚îÇ          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                   ‚îÇ                        ‚îÇ          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ                        ‚ñº          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ   CVE    ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ CodeBERT ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ  Query   ‚îÇ    ‚îÇ (HF API) ‚îÇ                    ‚îÇ  Gemini Prompt ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ  + Code Context‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## File Structure

```
hack/cve-triage/
‚îú‚îÄ‚îÄ main.go                      # Updated with index/triage subcommands
‚îú‚îÄ‚îÄ go.mod
‚îú‚îÄ‚îÄ go.sum
‚îú‚îÄ‚îÄ eck-index.json               # Generated index file (gitignored)
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ parser/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cve.go               # Existing
‚îÇ   ‚îú‚îÄ‚îÄ deps/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ checker.go           # Existing
‚îÇ   ‚îú‚îÄ‚îÄ vulncheck/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scanner.go           # Existing
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyzer.go          # Updated to include RAG results
‚îÇ   ‚îú‚îÄ‚îÄ github/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reporter.go          # Existing
‚îÇ   ‚îî‚îÄ‚îÄ rag/                     # NEW
‚îÇ       ‚îú‚îÄ‚îÄ chunker.go           # Go AST parsing into chunks
‚îÇ       ‚îú‚îÄ‚îÄ embedder.go          # CodeBERT via Hugging Face API
‚îÇ       ‚îú‚îÄ‚îÄ store.go             # File-based persistence
‚îÇ       ‚îú‚îÄ‚îÄ retriever.go         # Search and retrieval
‚îÇ       ‚îî‚îÄ‚îÄ types.go             # Shared types
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ RAG_CODEBERT_PLAN.md     # This file
```

## Implementation Details

### 1. Types (`types.go`)

```go
// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package rag

import "time"

// ChunkType represents the type of code chunk
type ChunkType string

const (
	ChunkTypeFunction ChunkType = "function"
	ChunkTypeMethod   ChunkType = "method"
	ChunkTypeType     ChunkType = "type"
	ChunkTypeConst    ChunkType = "const"
	ChunkTypeVar      ChunkType = "var"
)

// CodeChunk represents a semantic unit of code
type CodeChunk struct {
	ID        string    `json:"id"`         // Unique identifier (filepath:line)
	FilePath  string    `json:"file_path"`  // Path relative to repo root
	Package   string    `json:"package"`    // Go package name
	Name      string    `json:"name"`       // Function/type/var name
	Type      ChunkType `json:"type"`       // function, method, type, etc.
	Signature string    `json:"signature"`  // Function/type signature
	StartLine int       `json:"start_line"`
	EndLine   int       `json:"end_line"`
	Content   string    `json:"content"`    // Actual source code
	Imports   []string  `json:"imports"`    // Imports used in this file
}

// SearchResult represents a search result with similarity score
type SearchResult struct {
	Chunk      CodeChunk `json:"chunk"`
	Similarity float32   `json:"similarity"`
}

// Index represents the persisted index structure
type Index struct {
	Version    string       `json:"version"`
	CreatedAt  time.Time    `json:"created_at"`
	RepoPath   string       `json:"repo_path"`
	CommitHash string       `json:"commit_hash,omitempty"`
	ChunkCount int          `json:"chunk_count"`
	Chunks     []CodeChunk  `json:"chunks"`
	Embeddings [][]float32  `json:"embeddings"`
}

// IndexMetadata contains metadata without the heavy embeddings
type IndexMetadata struct {
	Version    string    `json:"version"`
	CreatedAt  time.Time `json:"created_at"`
	RepoPath   string    `json:"repo_path"`
	CommitHash string    `json:"commit_hash,omitempty"`
	ChunkCount int       `json:"chunk_count"`
}
```

### 2. Code Chunker (`chunker.go`)

```go
// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package rag

import (
	"bytes"
	"fmt"
	"go/ast"
	"go/parser"
	"go/printer"
	"go/token"
	"os"
	"path/filepath"
	"strings"
)

// Chunker parses Go source files into semantic chunks
type Chunker struct {
	repoPath string
	fset     *token.FileSet
}

// NewChunker creates a new code chunker
func NewChunker(repoPath string) *Chunker {
	return &Chunker{
		repoPath: repoPath,
		fset:     token.NewFileSet(),
	}
}

// ChunkRepository walks the repo and creates chunks from all Go files
func (c *Chunker) ChunkRepository() ([]CodeChunk, error) {
	var chunks []CodeChunk

	err := filepath.Walk(c.repoPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return nil // Skip errors
		}

		// Skip directories we don't want to index
		if info.IsDir() {
			name := info.Name()
			if name == "vendor" || name == "testdata" || name == ".git" ||
				name == "node_modules" || name == "hack" {
				return filepath.SkipDir
			}
			return nil
		}

		// Only process Go files (skip tests)
		if !strings.HasSuffix(path, ".go") {
			return nil
		}
		if strings.HasSuffix(path, "_test.go") {
			return nil
		}
		if strings.Contains(path, "zz_generated") {
			return nil
		}

		fileChunks, err := c.chunkFile(path)
		if err != nil {
			// Log but don't fail on parse errors
			fmt.Printf("Warning: failed to parse %s: %v\n", path, err)
			return nil
		}

		chunks = append(chunks, fileChunks...)
		return nil
	})

	return chunks, err
}

// chunkFile parses a single Go file into chunks
func (c *Chunker) chunkFile(filePath string) ([]CodeChunk, error) {
	src, err := os.ReadFile(filePath)
	if err != nil {
		return nil, err
	}

	file, err := parser.ParseFile(c.fset, filePath, src, parser.ParseComments)
	if err != nil {
		return nil, err
	}

	var chunks []CodeChunk

	// Get relative path
	relPath, _ := filepath.Rel(c.repoPath, filePath)

	// Extract imports for context
	var imports []string
	for _, imp := range file.Imports {
		imports = append(imports, strings.Trim(imp.Path.Value, `"`))
	}

	// Walk AST and extract functions, methods, types
	ast.Inspect(file, func(n ast.Node) bool {
		switch decl := n.(type) {
		case *ast.FuncDecl:
			chunk := c.extractFunction(relPath, file.Name.Name, decl, imports, src)
			if chunk != nil {
				chunks = append(chunks, *chunk)
			}

		case *ast.GenDecl:
			for _, spec := range decl.Specs {
				switch s := spec.(type) {
				case *ast.TypeSpec:
					chunk := c.extractType(relPath, file.Name.Name, s, decl, imports, src)
					if chunk != nil {
						chunks = append(chunks, *chunk)
					}
				}
			}
		}
		return true
	})

	return chunks, nil
}

// extractFunction creates a chunk from a function declaration
func (c *Chunker) extractFunction(filePath, pkgName string, fn *ast.FuncDecl, imports []string, src []byte) *CodeChunk {
	startPos := c.fset.Position(fn.Pos())
	endPos := c.fset.Position(fn.End())

	// Get the source code for this function
	content := string(src[fn.Pos()-1 : fn.End()-1])

	// Build signature
	var sig bytes.Buffer
	printer.Fprint(&sig, c.fset, fn.Type)

	chunkType := ChunkTypeFunction
	name := fn.Name.Name

	// Check if it's a method
	if fn.Recv != nil && len(fn.Recv.List) > 0 {
		chunkType = ChunkTypeMethod
		// Get receiver type name
		var recvType bytes.Buffer
		printer.Fprint(&recvType, c.fset, fn.Recv.List[0].Type)
		name = fmt.Sprintf("(%s).%s", recvType.String(), fn.Name.Name)
	}

	return &CodeChunk{
		ID:        fmt.Sprintf("%s:%d", filePath, startPos.Line),
		FilePath:  filePath,
		Package:   pkgName,
		Name:      name,
		Type:      chunkType,
		Signature: fmt.Sprintf("func %s%s", fn.Name.Name, sig.String()),
		StartLine: startPos.Line,
		EndLine:   endPos.Line,
		Content:   content,
		Imports:   imports,
	}
}

// extractType creates a chunk from a type declaration
func (c *Chunker) extractType(filePath, pkgName string, spec *ast.TypeSpec, decl *ast.GenDecl, imports []string, src []byte) *CodeChunk {
	startPos := c.fset.Position(decl.Pos())
	endPos := c.fset.Position(decl.End())

	content := string(src[decl.Pos()-1 : decl.End()-1])

	return &CodeChunk{
		ID:        fmt.Sprintf("%s:%d", filePath, startPos.Line),
		FilePath:  filePath,
		Package:   pkgName,
		Name:      spec.Name.Name,
		Type:      ChunkTypeType,
		Signature: fmt.Sprintf("type %s", spec.Name.Name),
		StartLine: startPos.Line,
		EndLine:   endPos.Line,
		Content:   content,
		Imports:   imports,
	}
}
```

### 3. CodeBERT Embedder (`embedder.go`)

```go
// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package rag

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"
)

const (
	// Hugging Face Inference API endpoint for CodeBERT
	defaultHFEndpoint = "https://api-inference.huggingface.co/pipeline/feature-extraction/microsoft/codebert-base"
	
	// Alternative: GraphCodeBERT (better for code understanding)
	graphCodeBERTEndpoint = "https://api-inference.huggingface.co/pipeline/feature-extraction/microsoft/graphcodebert-base"
	
	// Embedding dimensions
	embeddingDims = 768
	
	// Rate limiting
	maxRequestsPerMinute = 30
	requestDelay         = 2 * time.Second
)

// Embedder generates code embeddings using CodeBERT via Hugging Face API
type Embedder struct {
	apiKey   string
	endpoint string
	client   *http.Client
}

// EmbedderConfig configures the embedder
type EmbedderConfig struct {
	APIKey          string
	UseGraphCodeBERT bool // Use GraphCodeBERT instead of CodeBERT
	Timeout         time.Duration
}

// NewEmbedder creates a new CodeBERT embedder
func NewEmbedder(cfg EmbedderConfig) *Embedder {
	endpoint := defaultHFEndpoint
	if cfg.UseGraphCodeBERT {
		endpoint = graphCodeBERTEndpoint
	}

	timeout := cfg.Timeout
	if timeout == 0 {
		timeout = 60 * time.Second
	}

	return &Embedder{
		apiKey:   cfg.APIKey,
		endpoint: endpoint,
		client: &http.Client{
			Timeout: timeout,
		},
	}
}

// EmbedRequest is the request body for HF Inference API
type embedRequest struct {
	Inputs  string                 `json:"inputs"`
	Options map[string]interface{} `json:"options,omitempty"`
}

// Embed generates an embedding for a single text
func (e *Embedder) Embed(ctx context.Context, text string) ([]float32, error) {
	// Truncate long text (CodeBERT has 512 token limit)
	if len(text) > 2000 {
		text = text[:2000]
	}

	reqBody := embedRequest{
		Inputs: text,
		Options: map[string]interface{}{
			"wait_for_model": true,
		},
	}

	bodyBytes, err := json.Marshal(reqBody)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	req, err := http.NewRequestWithContext(ctx, "POST", e.endpoint, bytes.NewReader(bodyBytes))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Set("Authorization", "Bearer "+e.apiKey)
	req.Header.Set("Content-Type", "application/json")

	resp, err := e.client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("API error (status %d): %s", resp.StatusCode, string(body))
	}

	// Parse response - HF returns nested array for token embeddings
	// We need to average them to get a single vector
	var rawResponse [][]float32
	if err := json.NewDecoder(resp.Body).Decode(&rawResponse); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	// Average pool across tokens to get single embedding
	return averagePool(rawResponse), nil
}

// EmbedBatch generates embeddings for multiple texts with rate limiting
func (e *Embedder) EmbedBatch(ctx context.Context, texts []string, progressFn func(int, int)) ([][]float32, error) {
	embeddings := make([][]float32, len(texts))

	for i, text := range texts {
		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		default:
		}

		// Progress callback
		if progressFn != nil {
			progressFn(i+1, len(texts))
		}

		embedding, err := e.Embed(ctx, text)
		if err != nil {
			return nil, fmt.Errorf("failed to embed chunk %d: %w", i, err)
		}
		embeddings[i] = embedding

		// Rate limiting - HF free tier has limits
		if i < len(texts)-1 {
			time.Sleep(requestDelay)
		}
	}

	return embeddings, nil
}

// EmbedChunks embeds code chunks, formatting them appropriately
func (e *Embedder) EmbedChunks(ctx context.Context, chunks []CodeChunk, progressFn func(int, int)) ([][]float32, error) {
	texts := make([]string, len(chunks))
	for i, chunk := range chunks {
		texts[i] = formatChunkForEmbedding(chunk)
	}
	return e.EmbedBatch(ctx, texts, progressFn)
}

// formatChunkForEmbedding creates an embedding-friendly representation
func formatChunkForEmbedding(chunk CodeChunk) string {
	var sb strings.Builder

	// Include package for context
	sb.WriteString(fmt.Sprintf("// Package: %s\n", chunk.Package))

	// Include relevant imports
	relevantImports := filterRelevantImports(chunk.Imports)
	if len(relevantImports) > 0 {
		sb.WriteString("// Imports: ")
		sb.WriteString(strings.Join(relevantImports, ", "))
		sb.WriteString("\n")
	}

	// Include signature
	if chunk.Signature != "" {
		sb.WriteString(fmt.Sprintf("// %s\n", chunk.Signature))
	}

	// The actual code
	sb.WriteString(chunk.Content)

	return sb.String()
}

// filterRelevantImports keeps only interesting imports for embedding
func filterRelevantImports(imports []string) []string {
	var relevant []string
	for _, imp := range imports {
		// Skip standard library basics
		if imp == "fmt" || imp == "strings" || imp == "errors" ||
			imp == "context" || imp == "time" || imp == "sync" {
			continue
		}
		// Keep security/crypto related
		if strings.Contains(imp, "crypto") || strings.Contains(imp, "tls") ||
			strings.Contains(imp, "x509") || strings.Contains(imp, "ssh") {
			relevant = append(relevant, imp)
		}
		// Keep external packages
		if strings.Contains(imp, "github.com") || strings.Contains(imp, "k8s.io") ||
			strings.Contains(imp, "golang.org") {
			relevant = append(relevant, imp)
		}
	}
	return relevant
}

// averagePool computes the mean of token embeddings
func averagePool(tokenEmbeddings [][]float32) []float32 {
	if len(tokenEmbeddings) == 0 {
		return make([]float32, embeddingDims)
	}

	dims := len(tokenEmbeddings[0])
	result := make([]float32, dims)

	for _, embedding := range tokenEmbeddings {
		for j, val := range embedding {
			result[j] += val
		}
	}

	n := float32(len(tokenEmbeddings))
	for i := range result {
		result[i] /= n
	}

	return result
}
```

### 4. File-Based Store (`store.go`)

```go
// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package rag

import (
	"compress/gzip"
	"encoding/json"
	"fmt"
	"math"
	"os"
	"os/exec"
	"path/filepath"
	"sort"
	"strings"
	"time"
)

const (
	indexVersion = "1.0.0"
)

// Store manages the persisted index
type Store struct {
	filePath string
	index    *Index
}

// NewStore creates a new file-based store
func NewStore(filePath string) *Store {
	return &Store{
		filePath: filePath,
	}
}

// Exists checks if the index file exists
func (s *Store) Exists() bool {
	_, err := os.Stat(s.filePath)
	return err == nil
}

// Load loads the index from disk
func (s *Store) Load() error {
	file, err := os.Open(s.filePath)
	if err != nil {
		return fmt.Errorf("failed to open index file: %w", err)
	}
	defer file.Close()

	// Check if it's gzipped
	var reader interface {
		Read([]byte) (int, error)
	} = file

	if strings.HasSuffix(s.filePath, ".gz") {
		gzReader, err := gzip.NewReader(file)
		if err != nil {
			return fmt.Errorf("failed to create gzip reader: %w", err)
		}
		defer gzReader.Close()
		reader = gzReader
	}

	var index Index
	decoder := json.NewDecoder(reader.(interface{ Read([]byte) (int, error) }))
	if err := decoder.Decode(&index); err != nil {
		return fmt.Errorf("failed to decode index: %w", err)
	}

	s.index = &index
	return nil
}

// Save persists the index to disk
func (s *Store) Save() error {
	// Ensure directory exists
	dir := filepath.Dir(s.filePath)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	file, err := os.Create(s.filePath)
	if err != nil {
		return fmt.Errorf("failed to create index file: %w", err)
	}
	defer file.Close()

	// Use gzip for compression if filename ends with .gz
	var writer interface {
		Write([]byte) (int, error)
	} = file

	if strings.HasSuffix(s.filePath, ".gz") {
		gzWriter := gzip.NewWriter(file)
		defer gzWriter.Close()
		writer = gzWriter
	}

	encoder := json.NewEncoder(writer.(interface{ Write([]byte) (int, error) }))
	encoder.SetIndent("", "  ") // Pretty print for debugging

	if err := encoder.Encode(s.index); err != nil {
		return fmt.Errorf("failed to encode index: %w", err)
	}

	return nil
}

// BuildIndex creates a new index from chunks and embeddings
func (s *Store) BuildIndex(repoPath string, chunks []CodeChunk, embeddings [][]float32) error {
	commitHash := getGitCommitHash(repoPath)

	s.index = &Index{
		Version:    indexVersion,
		CreatedAt:  time.Now(),
		RepoPath:   repoPath,
		CommitHash: commitHash,
		ChunkCount: len(chunks),
		Chunks:     chunks,
		Embeddings: embeddings,
	}

	return nil
}

// Search finds the top-K most similar chunks to the query embedding
func (s *Store) Search(queryEmbedding []float32, topK int) []SearchResult {
	if s.index == nil || len(s.index.Chunks) == 0 {
		return nil
	}

	type scoredChunk struct {
		index      int
		similarity float32
	}

	// Calculate similarity for all chunks
	scores := make([]scoredChunk, len(s.index.Chunks))
	for i, embedding := range s.index.Embeddings {
		scores[i] = scoredChunk{
			index:      i,
			similarity: cosineSimilarity(queryEmbedding, embedding),
		}
	}

	// Sort by similarity (descending)
	sort.Slice(scores, func(i, j int) bool {
		return scores[i].similarity > scores[j].similarity
	})

	// Return top-K
	if topK > len(scores) {
		topK = len(scores)
	}

	results := make([]SearchResult, topK)
	for i := 0; i < topK; i++ {
		results[i] = SearchResult{
			Chunk:      s.index.Chunks[scores[i].index],
			Similarity: scores[i].similarity,
		}
	}

	return results
}

// GetMetadata returns index metadata without loading full embeddings
func (s *Store) GetMetadata() *IndexMetadata {
	if s.index == nil {
		return nil
	}
	return &IndexMetadata{
		Version:    s.index.Version,
		CreatedAt:  s.index.CreatedAt,
		RepoPath:   s.index.RepoPath,
		CommitHash: s.index.CommitHash,
		ChunkCount: s.index.ChunkCount,
	}
}

// ChunkCount returns the number of indexed chunks
func (s *Store) ChunkCount() int {
	if s.index == nil {
		return 0
	}
	return len(s.index.Chunks)
}

// cosineSimilarity calculates the cosine similarity between two vectors
func cosineSimilarity(a, b []float32) float32 {
	if len(a) != len(b) {
		return 0
	}

	var dotProduct, normA, normB float32
	for i := range a {
		dotProduct += a[i] * b[i]
		normA += a[i] * a[i]
		normB += b[i] * b[i]
	}

	if normA == 0 || normB == 0 {
		return 0
	}

	return dotProduct / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))
}

// getGitCommitHash gets the current git commit hash
func getGitCommitHash(repoPath string) string {
	cmd := exec.Command("git", "rev-parse", "HEAD")
	cmd.Dir = repoPath
	output, err := cmd.Output()
	if err != nil {
		return ""
	}
	return strings.TrimSpace(string(output))
}
```

### 5. Retriever (`retriever.go`)

```go
// Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
// or more contributor license agreements. Licensed under the Elastic License 2.0;
// you may not use this file except in compliance with the Elastic License 2.0.

package rag

import (
	"context"
	"fmt"
	"strings"
)

// Retriever orchestrates the RAG pipeline
type Retriever struct {
	store    *Store
	embedder *Embedder
	repoPath string
}

// NewRetriever creates a new retriever
func NewRetriever(indexPath, repoPath string, embedder *Embedder) *Retriever {
	return &Retriever{
		store:    NewStore(indexPath),
		embedder: embedder,
		repoPath: repoPath,
	}
}

// IndexExists checks if an index already exists
func (r *Retriever) IndexExists() bool {
	return r.store.Exists()
}

// LoadIndex loads an existing index from disk
func (r *Retriever) LoadIndex() error {
	return r.store.Load()
}

// BuildIndex creates a new index from the repository
func (r *Retriever) BuildIndex(ctx context.Context, progressFn func(stage string, current, total int)) error {
	// Stage 1: Chunk the codebase
	if progressFn != nil {
		progressFn("chunking", 0, 0)
	}

	chunker := NewChunker(r.repoPath)
	chunks, err := chunker.ChunkRepository()
	if err != nil {
		return fmt.Errorf("failed to chunk repository: %w", err)
	}

	if progressFn != nil {
		progressFn("chunking", len(chunks), len(chunks))
	}

	if len(chunks) == 0 {
		return fmt.Errorf("no code chunks found in repository")
	}

	// Stage 2: Generate embeddings
	embeddingProgress := func(current, total int) {
		if progressFn != nil {
			progressFn("embedding", current, total)
		}
	}

	embeddings, err := r.embedder.EmbedChunks(ctx, chunks, embeddingProgress)
	if err != nil {
		return fmt.Errorf("failed to generate embeddings: %w", err)
	}

	// Stage 3: Build and save index
	if progressFn != nil {
		progressFn("saving", 0, 1)
	}

	if err := r.store.BuildIndex(r.repoPath, chunks, embeddings); err != nil {
		return fmt.Errorf("failed to build index: %w", err)
	}

	if err := r.store.Save(); err != nil {
		return fmt.Errorf("failed to save index: %w", err)
	}

	if progressFn != nil {
		progressFn("saving", 1, 1)
	}

	return nil
}

// Retrieve finds code relevant to a CVE query
func (r *Retriever) Retrieve(ctx context.Context, query string, topK int) ([]SearchResult, error) {
	// Embed the query
	queryEmbedding, err := r.embedder.Embed(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("failed to embed query: %w", err)
	}

	// Search the index
	results := r.store.Search(queryEmbedding, topK)
	return results, nil
}

// RetrieveForCVE builds an optimized query from CVE info and retrieves
func (r *Retriever) RetrieveForCVE(ctx context.Context, cveDescription string, affectedPackages []string, topK int) ([]SearchResult, error) {
	query := buildCVEQuery(cveDescription, affectedPackages)
	return r.Retrieve(ctx, query, topK)
}

// buildCVEQuery creates an effective search query from CVE info
func buildCVEQuery(cveDescription string, affectedPackages []string) string {
	var parts []string

	// Add package names - important for matching imports
	for _, pkg := range affectedPackages {
		// Extract meaningful parts from package path
		pkgParts := strings.Split(pkg, "/")
		for _, p := range pkgParts {
			p = strings.ToLower(p)
			if p != "" && p != "x" && p != "golang.org" && p != "github.com" && p != "v2" && p != "v3" {
				parts = append(parts, p)
			}
		}
	}

	// Add CVE description keywords
	if cveDescription != "" {
		// Truncate and clean
		desc := cveDescription
		if len(desc) > 500 {
			desc = desc[:500]
		}
		parts = append(parts, desc)
	}

	return strings.Join(parts, " ")
}

// GetIndexMetadata returns metadata about the loaded index
func (r *Retriever) GetIndexMetadata() *IndexMetadata {
	return r.store.GetMetadata()
}

// FormatResultsForPrompt formats search results for the LLM prompt
func FormatResultsForPrompt(results []SearchResult, maxTokens int) string {
	if len(results) == 0 {
		return ""
	}

	var sb strings.Builder
	currentTokens := 0

	sb.WriteString("## Relevant ECK Source Code\n\n")
	sb.WriteString("The following code snippets were retrieved using CodeBERT semantic search.\n")
	sb.WriteString("These are the ECK code sections most similar to the CVE description and affected packages.\n\n")

	for i, result := range results {
		// Estimate tokens (rough: 4 chars per token)
		chunkTokens := len(result.Chunk.Content) / 4
		headerTokens := 100

		if currentTokens+chunkTokens+headerTokens > maxTokens {
			remaining := len(results) - i
			sb.WriteString(fmt.Sprintf("\n*... %d more relevant chunks omitted for length ...*\n", remaining))
			break
		}

		// Header
		sb.WriteString(fmt.Sprintf("### %s - `%s`\n\n", result.Chunk.FilePath, result.Chunk.Name))
		sb.WriteString(fmt.Sprintf("- **Type**: %s\n", result.Chunk.Type))
		sb.WriteString(fmt.Sprintf("- **Package**: %s\n", result.Chunk.Package))
		sb.WriteString(fmt.Sprintf("- **Lines**: %d-%d\n", result.Chunk.StartLine, result.Chunk.EndLine))
		sb.WriteString(fmt.Sprintf("- **Similarity**: %.3f\n", result.Similarity))

		if len(result.Chunk.Imports) > 0 {
			relevantImports := filterRelevantImports(result.Chunk.Imports)
			if len(relevantImports) > 0 {
				sb.WriteString(fmt.Sprintf("- **Key Imports**: %s\n", strings.Join(relevantImports, ", ")))
			}
		}
		sb.WriteString("\n")

		// Code block
		sb.WriteString("```go\n")
		sb.WriteString(result.Chunk.Content)
		if !strings.HasSuffix(result.Chunk.Content, "\n") {
			sb.WriteString("\n")
		}
		sb.WriteString("```\n\n")

		currentTokens += chunkTokens + headerTokens
	}

	return sb.String()
}

// FormatResultsAsText formats results for CLI output
func FormatResultsAsText(results []SearchResult) string {
	if len(results) == 0 {
		return "No relevant code chunks found.\n"
	}

	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("Found %d relevant code chunks:\n\n", len(results)))

	for i, result := range results {
		sb.WriteString(fmt.Sprintf("%d. %s - %s (similarity: %.3f)\n",
			i+1,
			result.Chunk.FilePath,
			result.Chunk.Name,
			result.Similarity))
		sb.WriteString(fmt.Sprintf("   Package: %s | Lines: %d-%d | Type: %s\n",
			result.Chunk.Package,
			result.Chunk.StartLine,
			result.Chunk.EndLine,
			result.Chunk.Type))

		// Show preview
		preview := result.Chunk.Content
		if len(preview) > 100 {
			preview = preview[:100] + "..."
		}
		preview = strings.ReplaceAll(preview, "\n", " ")
		preview = strings.Join(strings.Fields(preview), " ") // Normalize whitespace
		sb.WriteString(fmt.Sprintf("   Preview: %s\n\n", preview))
	}

	return sb.String()
}
```

### 6. CLI Integration

Update `main.go` to add index and RAG subcommands:

```go
var (
	// ... existing flags ...
	
	// RAG flags
	indexPath    string
	useRAG       bool
	ragTopK      int
	rebuildIndex bool
	hfAPIKey     string
)

func init() {
	// ... existing flags ...
	
	// RAG configuration
	rootCmd.Flags().StringVar(&indexPath, "index-path", "./eck-index.json", 
		"Path to the code index file")
	rootCmd.Flags().BoolVar(&useRAG, "use-rag", false, 
		"Use RAG to include relevant source code in analysis")
	rootCmd.Flags().IntVar(&ragTopK, "rag-top-k", 10, 
		"Number of code chunks to retrieve")
	rootCmd.Flags().BoolVar(&rebuildIndex, "rebuild-index", false, 
		"Rebuild the code index before triage")
	rootCmd.Flags().StringVar(&hfAPIKey, "hf-api-key", "", 
		"Hugging Face API key (or HF_API_KEY env)")
	
	// Add index subcommand
	rootCmd.AddCommand(indexCmd)
}

var indexCmd = &cobra.Command{
	Use:   "index",
	Short: "Build the code index for RAG",
	RunE:  runIndex,
}

func runIndex(cmd *cobra.Command, args []string) error {
	ctx := context.Background()
	
	// Get API key
	apiKey := hfAPIKey
	if apiKey == "" {
		apiKey = os.Getenv("HF_API_KEY")
	}
	if apiKey == "" {
		return fmt.Errorf("Hugging Face API key required (set HF_API_KEY or --hf-api-key)")
	}
	
	// Find repo path
	repoPath := repoPath
	if repoPath == "" {
		repoPath = filepath.Dir(goModPath)
	}
	
	fmt.Printf("üìö Building code index for %s...\n\n", repoPath)
	
	embedder := rag.NewEmbedder(rag.EmbedderConfig{
		APIKey: apiKey,
	})
	
	retriever := rag.NewRetriever(indexPath, repoPath, embedder)
	
	progressFn := func(stage string, current, total int) {
		switch stage {
		case "chunking":
			if total > 0 {
				fmt.Printf("   Parsed %d code chunks\n", total)
			} else {
				fmt.Printf("   Parsing Go source files...\n")
			}
		case "embedding":
			fmt.Printf("\r   Generating embeddings: %d/%d", current, total)
			if current == total {
				fmt.Println()
			}
		case "saving":
			if current == total {
				fmt.Printf("   Index saved to %s\n", indexPath)
			}
		}
	}
	
	if err := retriever.BuildIndex(ctx, progressFn); err != nil {
		return fmt.Errorf("failed to build index: %w", err)
	}
	
	meta := retriever.GetIndexMetadata()
	fmt.Printf("\n‚úÖ Index built successfully!\n")
	fmt.Printf("   Chunks: %d\n", meta.ChunkCount)
	fmt.Printf("   Commit: %s\n", meta.CommitHash[:8])
	
	return nil
}

func runTriage(cmd *cobra.Command, args []string) error {
	// ... existing code ...
	
	// RAG retrieval (if enabled)
	var ragResults []rag.SearchResult
	if useRAG {
		// Get HF API key
		apiKey := hfAPIKey
		if apiKey == "" {
			apiKey = os.Getenv("HF_API_KEY")
		}
		
		if apiKey == "" {
			fmt.Printf("\nüìö Skipping RAG: HF_API_KEY not configured\n")
		} else {
			fmt.Printf("\nüìö Retrieving relevant code...\n")
			
			embedder := rag.NewEmbedder(rag.EmbedderConfig{APIKey: apiKey})
			retriever := rag.NewRetriever(indexPath, eckRepoPath, embedder)
			
			// Check if we need to rebuild
			if rebuildIndex || !retriever.IndexExists() {
				fmt.Printf("   Building index (this may take a few minutes)...\n")
				err := retriever.BuildIndex(ctx, nil)
				if err != nil {
					fmt.Printf("   ‚ö†Ô∏è  Failed to build index: %v\n", err)
				}
			} else {
				if err := retriever.LoadIndex(); err != nil {
					fmt.Printf("   ‚ö†Ô∏è  Failed to load index: %v\n", err)
				}
			}
			
			// Retrieve relevant code
			if retriever.GetIndexMetadata() != nil {
				ragResults, err = retriever.RetrieveForCVE(ctx,
					cveInfo.Description,
					cveInfo.AffectedPackages,
					ragTopK)
				if err != nil {
					fmt.Printf("   ‚ö†Ô∏è  Retrieval failed: %v\n", err)
				} else {
					fmt.Printf("   ‚úÖ Retrieved %d relevant code chunks\n", len(ragResults))
				}
			}
		}
	}
	
	// Pass to LLM analyzer
	llmResult, err = analyzer.Analyze(ctx, cveInfo, depResult, vulnResult, ragResults)
	// ...
}
```

## CLI Usage

```bash
# Step 1: Build the index (run once or when code changes)
HF_API_KEY=your_key ./cve-triage index \
  --repo-path=/path/to/cloud-on-k8s \
  --index-path=./eck-index.json

# Step 2: Triage with RAG (uses existing index)
./cve-triage \
  --issue-body="CVE-2024-45337 affects golang.org/x/crypto..." \
  --use-rag \
  --index-path=./eck-index.json \
  --dry-run

# Or: Rebuild index + triage in one command
HF_API_KEY=your_key ./cve-triage \
  --issue-body="CVE-2024-..." \
  --use-rag \
  --rebuild-index \
  --repo-path=/path/to/eck \
  --dry-run
```

## GitHub Actions Integration

### Index Building Workflow (Periodic)

```yaml
# .github/workflows/build-code-index.yml
name: Build Code Index

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  push:
    branches: [main]
    paths:
      - 'pkg/**'
      - 'cmd/**'
  workflow_dispatch:

jobs:
  build-index:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-go@v5
        with:
          go-version: '1.24'

      - name: Build index
        env:
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          cd hack/cve-triage
          go run . index \
            --repo-path=../.. \
            --index-path=./eck-index.json

      - name: Upload index artifact
        uses: actions/upload-artifact@v4
        with:
          name: eck-code-index
          path: hack/cve-triage/eck-index.json
          retention-days: 7

      # Optional: Commit index to repo
      - name: Commit index
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add hack/cve-triage/eck-index.json
          git diff --staged --quiet || git commit -m "Update code index [skip ci]"
          git push
```

### CVE Triage Workflow (Updated)

```yaml
# .github/workflows/eck-cve-triage.yml
name: ECK CVE Triage

on:
  issues:
    types: [opened, labeled]

jobs:
  triage:
    if: contains(github.event.issue.labels.*.name, 'ECK')
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
        with:
          repository: elastic/cloud-on-k8s

      - uses: actions/setup-go@v5
        with:
          go-version: '1.24'

      - name: Install govulncheck
        run: go install golang.org/x/vuln/cmd/govulncheck@latest

      # Download pre-built index (if available)
      - name: Download code index
        uses: actions/download-artifact@v4
        with:
          name: eck-code-index
          path: hack/cve-triage
        continue-on-error: true

      - name: Run CVE Triage with RAG
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          HF_API_KEY: ${{ secrets.HF_API_KEY }}
        run: |
          cd hack/cve-triage
          go run . \
            --issue-number=${{ github.event.issue.number }} \
            --issue-body="${{ github.event.issue.body }}" \
            --repo-path=../.. \
            --use-rag \
            --rag-top-k=15 \
            --add-labels
```

## Cost & Performance

| Aspect | Estimate |
|--------|----------|
| **Index Build Time** | ~10-15 min (rate limited by HF API) |
| **Index File Size** | ~20-50 MB (gzipped: ~5-10 MB) |
| **Query Time** | <100ms (in-memory cosine similarity) |
| **HF API Cost** | Free tier: 30k requests/month |
| **Embedding Dimensions** | 768 (CodeBERT) |

## Secrets Required

| Secret | Where | Description |
|--------|-------|-------------|
| `HF_API_KEY` | ECK repo | Hugging Face API key |
| `GEMINI_API_KEY` | Security repo | Google Gemini API key |

## Comparison with Other Plans

| Aspect | Path A (Gemini) | Path A (CodeBERT) | Path B (ES) |
|--------|-----------------|-------------------|-------------|
| Embedding Model | text-embedding-004 | CodeBERT | ELSER + CodeBERT |
| Code Understanding | Generic | Code-specific ‚úÖ | Code-specific ‚úÖ |
| Persistence | None | File-based ‚úÖ | Elasticsearch |
| Infrastructure | None | None ‚úÖ | ES Cluster |
| Setup Effort | 2-3 days | 2-3 days | 3-5 days |
| Ongoing Cost | ~$0.005/run | Free (HF tier) ‚úÖ | ~$50-100/mo |

## Summary

This plan provides:
- ‚úÖ Code-specific embeddings (CodeBERT > generic text)
- ‚úÖ No infrastructure required (just Hugging Face API)
- ‚úÖ Persistent index (file-based, fast subsequent queries)
- ‚úÖ Simple to understand and maintain
- ‚úÖ Free tier friendly (HF has generous limits)
- ‚úÖ Pure Go implementation (integrated with existing tool)

